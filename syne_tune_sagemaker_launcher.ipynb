{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with HPO using Syne Tune SageMaker Launcher Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-pytorch-mnist'\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "local_dir = 'data'\n",
    "MNIST.mirrors = [\"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/\"]\n",
    "MNIST(\n",
    "    local_dir,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sagemaker_session.upload_data(path='data', bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point='mnist.py',\n",
    "    role=role,\n",
    "    py_version='py3',\n",
    "    framework_version='1.8.0',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.2xlarge',\n",
    "    hyperparameters={'epochs': 1, 'backend': 'gloo'},\n",
    "    source_dir='source_dir'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syne_tune import search_space\n",
    "from syne_tune.optimizer.baselines import RandomSearch\n",
    "from syne_tune.backend.sagemaker_backend.sagemaker_backend import SagemakerBackend\n",
    "from syne_tune.stopping_criterion import StoppingCriterion\n",
    "from syne_tune.tuner import Tuner\n",
    "from syne_tune.remote.remote_launcher import RemoteLauncher\n",
    "from syne_tune.experiments import load_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_space = {\n",
    "    'lr': search_space.loguniform(0.001, 0.1),\n",
    "    'batch-size': search_space.choice([32, 64, 128, 256, 512])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = RandomSearch(\n",
    "    config_space=config_space,\n",
    "    mode='min',\n",
    "    metric='test_loss',\n",
    "    random_seed=31415927\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = SagemakerBackend(\n",
    "    sm_estimator=estimator,\n",
    "    metrics_names=['test_loss'],\n",
    "    inputs={'training': inputs}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_criterion = StoppingCriterion(max_wallclock_time=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RemoteLauncher(\n",
    "    tuner=Tuner(\n",
    "        scheduler=scheduler,\n",
    "        backend=backend,\n",
    "        stop_criterion=stop_criterion,\n",
    "        n_workers=3,\n",
    "        sleep_time=5.0,\n",
    "        tuner_name='hpo-hyperband'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-09 16:52:53 Starting - Starting the training job...\n",
      "2022-02-09 16:52:56 Starting - Launching requested ML instancesProfilerReport-1644425572: InProgress\n",
      ".........\n",
      "2022-02-09 16:54:51 Starting - Preparing the instances for training...\n",
      "2022-02-09 16:55:17 Downloading - Downloading input data...\n",
      "2022-02-09 16:55:51 Training - Downloading the training image.....\u001b[34m2022-02-09 16:56:33,138 sagemaker-training-toolkit INFO     Imported framework sagemaker_mxnet_container.training\u001b[0m\n",
      "\u001b[34m2022-02-09 16:56:33,141 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-02-09 16:56:33,153 sagemaker_mxnet_container.training INFO     MXNet training environment: {'SM_HOSTS': '[\"algo-1\"]', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_HPS': '{\"no_tuner_logging\":false,\"store_logs\":false,\"tuner_path\":\"tuner/\"}', 'SM_USER_ENTRY_POINT': 'remote_main.py', 'SM_FRAMEWORK_PARAMS': '{}', 'SM_RESOURCE_CONFIG': '{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}', 'SM_INPUT_DATA_CONFIG': '{}', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_CHANNELS': '[]', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODULE_NAME': 'remote_main', 'SM_LOG_LEVEL': '20', 'SM_FRAMEWORK_MODULE': 'sagemaker_mxnet_container.training:main', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_NUM_CPUS': '4', 'SM_NUM_GPUS': '0', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_MODULE_DIR': 's3://sagemaker-ap-northeast-1-861558531758/hpo-hyperband-2022-02-09-16-52-39-734/source/sourcedir.tar.gz', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"no_tuner_logging\":false,\"store_logs\":false,\"tuner_path\":\"tuner/\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"hpo-hyperband-2022-02-09-16-52-39-734\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-northeast-1-861558531758/hpo-hyperband-2022-02-09-16-52-39-734/source/sourcedir.tar.gz\",\"module_name\":\"remote_main\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"remote_main.py\"}', 'SM_USER_ARGS': '[\"--no_tuner_logging\",\"False\",\"--store_logs\",\"False\",\"--tuner_path\",\"tuner/\"]', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_HP_NO_TUNER_LOGGING': 'false', 'SM_HP_STORE_LOGS': 'false', 'SM_HP_TUNER_PATH': 'tuner/'}\u001b[0m\n",
      "\u001b[34m2022-02-09 16:56:33,578 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-02-09 16:56:34,198 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-02-09 16:56:34,212 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-02-09 16:56:34,223 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_mxnet_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"no_tuner_logging\": false,\n",
      "        \"store_logs\": false,\n",
      "        \"tuner_path\": \"tuner/\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"hpo-hyperband-2022-02-09-16-52-39-734\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-northeast-1-861558531758/hpo-hyperband-2022-02-09-16-52-39-734/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"remote_main\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"remote_main.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"no_tuner_logging\":false,\"store_logs\":false,\"tuner_path\":\"tuner/\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=remote_main.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=remote_main\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_mxnet_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-northeast-1-861558531758/hpo-hyperband-2022-02-09-16-52-39-734/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"no_tuner_logging\":false,\"store_logs\":false,\"tuner_path\":\"tuner/\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"hpo-hyperband-2022-02-09-16-52-39-734\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-northeast-1-861558531758/hpo-hyperband-2022-02-09-16-52-39-734/source/sourcedir.tar.gz\",\"module_name\":\"remote_main\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"remote_main.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--no_tuner_logging\",\"False\",\"--store_logs\",\"False\",\"--tuner_path\",\"tuner/\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_NO_TUNER_LOGGING=false\u001b[0m\n",
      "\u001b[34mSM_HP_STORE_LOGS=false\u001b[0m\n",
      "\u001b[34mSM_HP_TUNER_PATH=tuner/\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python36.zip:/usr/local/lib/python3.6:/usr/local/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.6 remote_main.py --no_tuner_logging False --store_logs False --tuner_path tuner/\u001b[0m\n",
      "\u001b[34mINFO:root:load tuner from path tuner/\u001b[0m\n",
      "\u001b[34mINFO:root:starting remote tuning\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:results of trials will be saved on /opt/ml/checkpoints/hpo-hyperband-2022-02-09-16-52-39-734\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.optimizer.schedulers.searchers.bayesopt.utils.debug_log:[0: random]\u001b[0m\n",
      "\u001b[34mlr: 0.010000000000000004\u001b[0m\n",
      "\u001b[34mbatch-size: 32\u001b[0m\n",
      "\u001b[34mINFO:root:Trial 0 will checkpoint results to s3://sagemaker-ap-northeast-1-861558531758/syne-tune/hpo-hyperband-2022-02-09-16-52-39-734/0/.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker:Creating training-job with name: hpo-hyperband-2022-02-09-16-52-39-734-0\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.backend.sagemaker_backend.sagemaker_backend:scheduled hpo-hyperband-2022-02-09-16-52-39-734-0 for trial-id 0\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:(trial 0) - scheduled config {'lr': 0.010000000000000004, 'batch-size': 32}\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.optimizer.schedulers.searchers.bayesopt.utils.debug_log:[1: random]\u001b[0m\n",
      "\u001b[34mlr: 0.029382310188815524\u001b[0m\n",
      "\u001b[34mbatch-size: 512\u001b[0m\n",
      "\u001b[34mINFO:root:Trial 1 will checkpoint results to s3://sagemaker-ap-northeast-1-861558531758/syne-tune/hpo-hyperband-2022-02-09-16-52-39-734/1/.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker:Creating training-job with name: hpo-hyperband-2022-02-09-16-52-39-734-1\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.backend.sagemaker_backend.sagemaker_backend:scheduled hpo-hyperband-2022-02-09-16-52-39-734-1 for trial-id 1\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:(trial 1) - scheduled config {'lr': 0.029382310188815524, 'batch-size': 512}\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.optimizer.schedulers.searchers.bayesopt.utils.debug_log:[2: random]\u001b[0m\n",
      "\u001b[34mlr: 0.046364110423598885\u001b[0m\n",
      "\u001b[34mbatch-size: 512\u001b[0m\n",
      "\u001b[34mINFO:root:Trial 2 will checkpoint results to s3://sagemaker-ap-northeast-1-861558531758/syne-tune/hpo-hyperband-2022-02-09-16-52-39-734/2/.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker:Creating training-job with name: hpo-hyperband-2022-02-09-16-52-39-734-2\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.backend.sagemaker_backend.sagemaker_backend:scheduled hpo-hyperband-2022-02-09-16-52-39-734-2 for trial-id 2\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:(trial 2) - scheduled config {'lr': 0.046364110423598885, 'batch-size': 512}\u001b[0m\n",
      "\n",
      "2022-02-09 16:56:51 Training - Training image download completed. Training in progress.\u001b[34mINFO:syne_tune.tuner:tuning status (last metric is reported)\n",
      " trial_id      status  iter  batch-size        lr\n",
      "        0  InProgress     0          32  0.010000\n",
      "        1  InProgress     0         512  0.029382\n",
      "        2  InProgress     0         512  0.046364\u001b[0m\n",
      "\u001b[34m3 trials running, 0 finished (0 until the end), 35.80s wallclock-time\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:tuning status (last metric is reported)\n",
      " trial_id      status  iter  batch-size        lr\n",
      "        0  InProgress     0          32  0.010000\n",
      "        1  InProgress     0         512  0.029382\n",
      "        2  InProgress     0         512  0.046364\u001b[0m\n",
      "\u001b[34m3 trials running, 0 finished (0 until the end), 67.77s wallclock-time\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:tuning status (last metric is reported)\n",
      " trial_id      status  iter  batch-size        lr\n",
      "        0  InProgress     0          32  0.010000\n",
      "        1  InProgress     0         512  0.029382\n",
      "        2  InProgress     0         512  0.046364\u001b[0m\n",
      "\u001b[34m3 trials running, 0 finished (0 until the end), 99.89s wallclock-time\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:tuning status (last metric is reported)\n",
      " trial_id      status  iter  batch-size        lr\n",
      "        0  InProgress     0          32  0.010000\n",
      "        1  InProgress     0         512  0.029382\n",
      "        2  InProgress     0         512  0.046364\u001b[0m\n",
      "\u001b[34m3 trials running, 0 finished (0 until the end), 131.11s wallclock-time\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:tuning status (last metric is reported)\n",
      " trial_id      status  iter  batch-size        lr\n",
      "        0  InProgress     0          32  0.010000\n",
      "        1  InProgress     0         512  0.029382\n",
      "        2  InProgress     0         512  0.046364\u001b[0m\n",
      "\u001b[34m3 trials running, 0 finished (0 until the end), 164.05s wallclock-time\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:tuning status (last metric is reported)\n",
      " trial_id      status  iter  batch-size        lr  test_loss  worker-time  worker-cost\n",
      "        0  InProgress     1          32  0.010000   0.139489    17.229520     0.002278\n",
      "        1  InProgress     0         512  0.029382          -            -            -\n",
      "        2  InProgress     3         512  0.046364   0.145395    35.657793     0.004715\u001b[0m\n",
      "\u001b[34m3 trials running, 0 finished (0 until the end), 197.76s wallclock-time, 0.00699287799906889$ estimated cost\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:tuning status (last metric is reported)\n",
      " trial_id      status  iter  batch-size        lr  test_loss  worker-time  worker-cost\n",
      "        0  InProgress     3          32  0.010000   0.070303    51.024639     0.006747\n",
      "        1  InProgress     0         512  0.029382          -            -            -\n",
      "        2  InProgress     5         512  0.046364   0.098324    59.373562     0.007851\u001b[0m\n",
      "\u001b[34m3 trials running, 0 finished (0 until the end), 232.47s wallclock-time, 0.014597095490148891$ estimated cost\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:tuning status (last metric is reported)\n",
      " trial_id      status  iter  batch-size        lr  test_loss  worker-time  worker-cost\n",
      "        0  InProgress     5          32  0.010000   0.056953    84.619168     0.011189\n",
      "        1  InProgress     3         512  0.029382   0.188490    32.970560     0.004359\n",
      "        2  InProgress     8         512  0.046364   0.074375    94.750619     0.012528\u001b[0m\n",
      "\u001b[34m3 trials running, 0 finished (0 until the end), 266.77s wallclock-time, 0.02807611245218112$ estimated cost\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:Trial trial_id 2 completed.\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.optimizer.schedulers.searchers.searcher:Update for trial_id 2: metric = 0.066\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.optimizer.schedulers.searchers.bayesopt.utils.debug_log:[3: random]\u001b[0m\n",
      "\u001b[34mlr: 0.08765875586108406\u001b[0m\n",
      "\u001b[34mbatch-size: 256\u001b[0m\n",
      "\u001b[34mINFO:root:Trial 3 will checkpoint results to s3://sagemaker-ap-northeast-1-861558531758/syne-tune/hpo-hyperband-2022-02-09-16-52-39-734/3/.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker:Creating training-job with name: hpo-hyperband-2022-02-09-16-52-39-734-3\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.backend.sagemaker_backend.sagemaker_backend:scheduled hpo-hyperband-2022-02-09-16-52-39-734-3 for trial-id 3\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:(trial 3) - scheduled config {'lr': 0.08765875586108406, 'batch-size': 256}\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:tuning status (last metric is reported)\n",
      " trial_id      status  iter  batch-size        lr  test_loss  worker-time  worker-cost\n",
      "        0  InProgress     7          32  0.010000   0.046059   118.151375     0.015622\n",
      "        1  InProgress     6         512  0.029382   0.117243    64.742956     0.008560\n",
      "        2   Completed    10         512  0.046364   0.066290   118.382220     0.015653\n",
      "        3  InProgress     0         256  0.087659          -            -            -\u001b[0m\n",
      "\u001b[34m3 trials running, 1 finished (1 until the end), 298.64s wallclock-time, 0.03983545512942334$ estimated cost\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:tuning status (last metric is reported)\n",
      " trial_id      status  iter  batch-size        lr  test_loss  worker-time  worker-cost\n",
      "        0  InProgress     9          32  0.010000   0.044394   151.774119     0.020068\n",
      "        1  InProgress    10         512  0.029382   0.083640   105.858185     0.013997\n",
      "        2   Completed    10         512  0.046364   0.066290   118.382220     0.015653\n",
      "        3  InProgress     0         256  0.087659          -            -            -\u001b[0m\n",
      "\u001b[34m3 trials running, 1 finished (1 until the end), 333.63s wallclock-time, 0.04971747605120334$ estimated cost\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:Trial trial_id 0 completed.\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.optimizer.schedulers.searchers.searcher:Update for trial_id 0: metric = 0.045\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:Trial trial_id 1 completed.\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.optimizer.schedulers.searchers.searcher:Update for trial_id 1: metric = 0.084\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.optimizer.schedulers.searchers.bayesopt.utils.debug_log:[4: random]\u001b[0m\n",
      "\u001b[34mlr: 0.0028034142150185812\u001b[0m\n",
      "\u001b[34mbatch-size: 32\u001b[0m\n",
      "\u001b[34mINFO:root:Trial 4 will checkpoint results to s3://sagemaker-ap-northeast-1-861558531758/syne-tune/hpo-hyperband-2022-02-09-16-52-39-734/4/.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker:Creating training-job with name: hpo-hyperband-2022-02-09-16-52-39-734-4\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.backend.sagemaker_backend.sagemaker_backend:scheduled hpo-hyperband-2022-02-09-16-52-39-734-4 for trial-id 4\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:(trial 4) - scheduled config {'lr': 0.0028034142150185812, 'batch-size': 32}\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.optimizer.schedulers.searchers.bayesopt.utils.debug_log:[5: random]\u001b[0m\n",
      "\u001b[34mlr: 0.023041304913462065\u001b[0m\n",
      "\u001b[34mbatch-size: 64\u001b[0m\n",
      "\u001b[34mINFO:root:Trial 5 will checkpoint results to s3://sagemaker-ap-northeast-1-861558531758/syne-tune/hpo-hyperband-2022-02-09-16-52-39-734/5/.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker:Creating training-job with name: hpo-hyperband-2022-02-09-16-52-39-734-5\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.backend.sagemaker_backend.sagemaker_backend:scheduled hpo-hyperband-2022-02-09-16-52-39-734-5 for trial-id 5\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:(trial 5) - scheduled config {'lr': 0.023041304913462065, 'batch-size': 64}\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:tuning status (last metric is reported)\n",
      " trial_id      status  iter  batch-size        lr  test_loss  worker-time  worker-cost\n",
      "        0   Completed    10          32  0.010000   0.044625   168.593697     0.022292\n",
      "        1   Completed    10         512  0.029382   0.083640   105.858185     0.013997\n",
      "        2   Completed    10         512  0.046364   0.066290   118.382220     0.015653\n",
      "        3  InProgress     0         256  0.087659          -            -            -\n",
      "        4  InProgress     0          32  0.002803          -            -            -\n",
      "        5  InProgress     0          64  0.023041          -            -            -\u001b[0m\n",
      "\u001b[34m3 trials running, 3 finished (3 until the end), 368.13s wallclock-time, 0.05194139793296222$ estimated cost\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:tuning status (last metric is reported)\n",
      " trial_id      status  iter  batch-size        lr  test_loss  worker-time  worker-cost\n",
      "        0   Completed    10          32  0.010000   0.044625   168.593697     0.022292\n",
      "        1   Completed    10         512  0.029382   0.083640   105.858185     0.013997\n",
      "        2   Completed    10         512  0.046364   0.066290   118.382220     0.015653\n",
      "        3  InProgress     0         256  0.087659          -            -            -\n",
      "        4  InProgress     0          32  0.002803          -            -            -\n",
      "        5  InProgress     0          64  0.023041          -            -            -\u001b[0m\n",
      "\u001b[34m3 trials running, 3 finished (3 until the end), 399.62s wallclock-time, 0.05194139793296222$ estimated cost\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:tuning status (last metric is reported)\n",
      " trial_id      status  iter  batch-size        lr  test_loss  worker-time  worker-cost\n",
      "        0   Completed    10          32  0.010000   0.044625   168.593697     0.022292\n",
      "        1   Completed    10         512  0.029382   0.083640   105.858185     0.013997\n",
      "        2   Completed    10         512  0.046364   0.066290   118.382220     0.015653\n",
      "        3  InProgress     0         256  0.087659          -            -            -\n",
      "        4  InProgress     0          32  0.002803          -            -            -\n",
      "        5  InProgress     0          64  0.023041          -            -            -\u001b[0m\n",
      "\u001b[34m3 trials running, 3 finished (3 until the end), 430.93s wallclock-time, 0.05194139793296222$ estimated cost\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:tuning status (last metric is reported)\n",
      " trial_id      status  iter  batch-size        lr  test_loss  worker-time  worker-cost\n",
      "        0   Completed    10          32  0.010000   0.044625   168.593697     0.022292\n",
      "        1   Completed    10         512  0.029382   0.083640   105.858185     0.013997\n",
      "        2   Completed    10         512  0.046364   0.066290   118.382220     0.015653\n",
      "        3  InProgress     0         256  0.087659          -            -            -\n",
      "        4  InProgress     0          32  0.002803          -            -            -\n",
      "        5  InProgress     0          64  0.023041          -            -            -\u001b[0m\n",
      "\u001b[34m3 trials running, 3 finished (3 until the end), 462.67s wallclock-time, 0.05194139793296222$ estimated cost\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:tuning status (last metric is reported)\n",
      " trial_id      status  iter  batch-size        lr  test_loss  worker-time  worker-cost\n",
      "        0   Completed    10          32  0.010000   0.044625   168.593697     0.022292\n",
      "        1   Completed    10         512  0.029382   0.083640   105.858185     0.013997\n",
      "        2   Completed    10         512  0.046364   0.066290   118.382220     0.015653\n",
      "        3  InProgress     0         256  0.087659          -            -            -\n",
      "        4  InProgress     0          32  0.002803          -            -            -\n",
      "        5  InProgress     0          64  0.023041          -            -            -\u001b[0m\n",
      "\u001b[34m3 trials running, 3 finished (3 until the end), 494.20s wallclock-time, 0.05194139793296222$ estimated cost\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:tuning status (last metric is reported)\n",
      " trial_id      status  iter  batch-size        lr  test_loss  worker-time  worker-cost\n",
      "        0   Completed    10          32  0.010000   0.044625   168.593697     0.022292\n",
      "        1   Completed    10         512  0.029382   0.083640   105.858185     0.013997\n",
      "        2   Completed    10         512  0.046364   0.066290   118.382220     0.015653\n",
      "        3  InProgress     0         256  0.087659          -            -            -\n",
      "        4  InProgress     1          32  0.002803   0.302381    16.654545     0.002202\n",
      "        5  InProgress     1          64  0.023041   0.125555    15.444908     0.002042\u001b[0m\n",
      "\u001b[34m3 trials running, 3 finished (3 until the end), 531.20s wallclock-time, 0.056185658992967784$ estimated cost\u001b[0m\n",
      "\u001b[34mINFO:syne_tune.tuner:tuning status (last metric is reported)\n",
      " trial_id      status  iter  batch-size        lr  test_loss  worker-time  worker-cost\n",
      "        0   Completed    10          32  0.010000   0.044625   168.593697     0.022292\n",
      "        1   Completed    10         512  0.029382   0.083640   105.858185     0.013997\n",
      "        2   Completed    10         512  0.046364   0.066290   118.382220     0.015653\n",
      "        3  InProgress     4         256  0.087659   0.060968    44.578027     0.005894\n",
      "        4  InProgress     3          32  0.002803   0.140773    48.551146     0.006420\n",
      "        5  InProgress     4          64  0.023041   0.059904    59.035779     0.007806\u001b[0m\n",
      "\u001b[34m3 trials running, 3 finished (3 until the end), 565.20s wallclock-time, 0.07206098610438778$ estimated cost\u001b[0m\n",
      "\n",
      "2022-02-09 17:06:54 Uploading - Uploading generated training model\n",
      "2022-02-09 17:06:54 Completed - Training job completed\n",
      "Training seconds: 690\n",
      "Billable seconds: 690\n"
     ]
    }
   ],
   "source": [
    "tuner.run()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-northeast-1:102112518831:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
